import asyncio
import functools
import re
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import akshare as ak
import pandas as pd

from src.utils.config_loader import ConfigLoader
from src.utils.logger import logger
from src.collector.sources.efinance_source import EfinanceSource
from src.collector.sources.akshare_source import AkshareSource
from src.collector.sources.tencent_source import TencentSource

from tenacity import retry, stop_after_attempt, wait_exponential


@dataclass
class CircuitBreakerState:
    """
    ğŸ”§ ä¼˜åŒ–åçš„ç†”æ–­å™¨çŠ¶æ€
    - è¦æ±‚è¿ç»­3æ¬¡å¤±è´¥æ‰ç†”æ–­
    - 30ç§’åè¿›å…¥åŠå¼€çŠ¶æ€ï¼Œå…è®¸ä¸€æ¬¡å°è¯•
    """
    failure_count: int = 0
    is_open: bool = False  # True = ç†”æ–­ä¸­
    last_failure_time: float = 0.0

    # é…ç½®
    FAILURE_THRESHOLD: int = 3  # è¿ç»­å¤±è´¥Næ¬¡æ‰ç†”æ–­
    RECOVERY_TIMEOUT: float = 30.0  # ç†”æ–­å30ç§’è¿›å…¥åŠå¼€çŠ¶æ€


class DataCollector:
    def __init__(self):
        # GitHub Actions runners / Standard Cloud Instances (2-4 vCPUs)
        self.executor = ThreadPoolExecutor(max_workers=16)
        self.config = ConfigLoader().config

        # Priority: Tencent -> Efinance -> AkShare
        self.sources = [TencentSource(), EfinanceSource(), AkshareSource()]

        # ğŸ”§ ä¼˜åŒ–: ä½¿ç”¨ç»“æ„åŒ–çš„ç†”æ–­å™¨çŠ¶æ€ï¼Œæ›¿ä»£ç®€å•çš„disabled set
        self._circuit_breakers: Dict[str, CircuitBreakerState] = {
            source.get_source_name(): CircuitBreakerState()
            for source in self.sources
        }

    def _should_skip_source(self, source_name: str) -> bool:
        """
        æ£€æŸ¥æ•°æ®æºæ˜¯å¦åº”è¯¥è·³è¿‡ï¼ˆç†”æ–­ä¸­ä¸”æœªåˆ°æ¢å¤æ—¶é—´ï¼‰
        """
        cb = self._circuit_breakers.get(source_name)
        if not cb:
            return False

        if not cb.is_open:
            return False

        # æ£€æŸ¥æ˜¯å¦åˆ°äº†åŠå¼€æ¢å¤æ—¶é—´
        elapsed = time.time() - cb.last_failure_time
        if elapsed >= cb.RECOVERY_TIMEOUT:
            logger.info(f"Circuit Breaker: {source_name} entering half-open state (trying recovery)")
            return False  # å…è®¸å°è¯•

        return True  # ä»åœ¨ç†”æ–­ä¸­

    def _record_success(self, source_name: str):
        """è®°å½•æˆåŠŸï¼Œé‡ç½®ç†”æ–­å™¨"""
        cb = self._circuit_breakers.get(source_name)
        if cb:
            if cb.is_open:
                logger.info(f"Circuit Breaker: {source_name} recovered successfully")
            cb.failure_count = 0
            cb.is_open = False

    def _record_failure(self, source_name: str):
        """è®°å½•å¤±è´¥ï¼Œå¯èƒ½è§¦å‘ç†”æ–­"""
        cb = self._circuit_breakers.get(source_name)
        if not cb:
            return

        cb.failure_count += 1
        cb.last_failure_time = time.time()

        if cb.failure_count >= cb.FAILURE_THRESHOLD:
            cb.is_open = True
            logger.warning(
                f"Circuit Breaker: {source_name} OPEN after {cb.failure_count} consecutive failures. "
                f"Will retry in {cb.RECOVERY_TIMEOUT}s."
            )
        else:
            logger.info(
                f"Circuit Breaker: {source_name} failure {cb.failure_count}/{cb.FAILURE_THRESHOLD}"
            )

    async def _run_blocking(self, func, *args, **kwargs):
        """
        Helper to run blocking calls in a thread executor with smart retry logic and timeout.
        """
        loop = asyncio.get_running_loop()
        timeout = kwargs.pop('timeout', 5) # Default 5s timeout (Fail Fast)
        
        @retry(
            stop=stop_after_attempt(3), 
            wait=wait_exponential(multiplier=1, min=1, max=10),
            reraise=True
        )
        def retriable_func():
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.warning(f"API Call {func.__name__} failed: {e}. Retrying...")
                raise e

        try:
            return await asyncio.wait_for(
                loop.run_in_executor(self.executor, retriable_func),
                timeout=timeout
            )
        except asyncio.TimeoutError:
            logger.error(f"Command {func.__name__} timed out after {timeout}s.")
            raise
        except Exception as e:
            # logger.error(f"Command {func.__name__} failed definitively after retries.")
            raise e

    async def _fetch_with_fallback(self, method_name: str, *args, **kwargs) -> Any:
        """
        Try to fetch data from sources in priority order.
        ğŸ”§ ä¼˜åŒ–: ä½¿ç”¨æ”¹è¿›çš„ç†”æ–­å™¨é€»è¾‘
        """
        last_exception = None
        for source in self.sources:
            source_name = source.get_source_name()

            # ç†”æ–­å™¨æ£€æŸ¥
            if self._should_skip_source(source_name):
                continue

            try:
                func = getattr(source, method_name)
                # Run sync source method in thread pool
                result = await self._run_blocking(func, *args, **kwargs)

                # Check for validity
                if result is not None:
                    if isinstance(result, pd.DataFrame) and result.empty:
                        continue  # Try next source if Empty DataFrame
                    # æˆåŠŸï¼é‡ç½®ç†”æ–­å™¨
                    self._record_success(source_name)
                    return result
            except Exception as e:
                logger.warning(f"Source {source_name} failed for {method_name}: {e}")
                # è®°å½•å¤±è´¥ï¼ˆå¯èƒ½è§¦å‘ç†”æ–­ï¼‰
                self._record_failure(source_name)
                last_exception = e
                continue

        logger.error(f"All sources failed for {method_name}.")
        return None

    def _get_dynamic_start_date(self, days_lookback: int = 60) -> str:
        return (datetime.now() - timedelta(days=days_lookback)).strftime("%Y%m%d")

    async def get_market_breadth(self) -> str:
        """
        Fetches market breadth (Rise/Fall ratio).
        """
        logger.info("Fetching market breadth...")
        # Try abstraction first, or fallback to current manual logic if needed?
        # Current logic is specific with 'stock_zh_a_spot_em'.
        # Let's rely on AkShareSource's fetch_market_breadth for simplicity?
        # But wait, AkshareSource's impl was a placeholder that returns string.
        # The original logic calculated it from spot data.
        # Let's stick to using fetch_market_breadth from interfaces.
        res = await self._fetch_with_fallback('fetch_market_breadth')
        return res if res else "Unknown"

    async def get_north_funds(self) -> float:
        """
        Fetches Real-time Northbound Fund Net Inflow (Unit: 100 million).
        Kept specific to AkShare for now as it's specialized.
        """
        logger.info("Fetching Northbound funds...")
        try:
            df = await self._run_blocking(ak.stock_hsgt_fund_flow_summary_em)
            if df is None or df.empty:
                return 0.0

            mask = df.astype(str).apply(lambda x: x.str.contains('åŒ—å‘')).any(axis=1)
            north_rows = df[mask]
            
            if north_rows.empty:
                return 0.0
            
            # Heuristic to find value column
            value_col = next((col for col in df.columns if 'å‡€æµå…¥' in str(col)), None)
            
            if not value_col:
                raw_val = north_rows.iloc[0, 1]
            else:
                raw_val = north_rows.iloc[0][value_col]

            val_str = str(raw_val)
            val_clean = re.sub(r'[^\d\.\-]', '', val_str)
            
            try:
                return round(float(val_clean), 2)
            except ValueError:
                return 0.0

        except Exception as e:
            logger.error(f"Error fetching North funds: {e}")
            return 0.0

    async def get_indices(self) -> Dict[str, Dict]:
        """
        Fetches major indices.
        """
        try:
             df = await self._run_blocking(ak.stock_zh_index_spot_sina)
             target_map = {
                 "ä¸Šè¯æŒ‡æ•°": "sh000001",
                 "æ·±è¯æˆæŒ‡": "sz399001", 
                 "åˆ›ä¸šæ¿æŒ‡": "sz399006"
             }
             results = {}
             for name in target_map.keys():
                 row = df[df['åç§°'] == name]
                 if not row.empty:
                     try:
                         results[name] = {
                             "current": float(row.iloc[0]['æœ€æ–°ä»·']),
                             "change_pct": float(row.iloc[0]['æ¶¨è·Œå¹…'])
                         }
                     except (ValueError, KeyError):
                         continue
             return results
        except Exception as e:
            logger.error(f"Failed to fetch indices: {e}")
            return {}

    async def get_macro_news(self) -> Dict[str, List[str]]:
        """
        Fetches macro news.
        """
        logger.info("Fetching macro news...")
        result: Dict[str, List[str]] = {"telegraph": [], "ai_tech": []}
        
        try:
            today_str = datetime.now().strftime('%Y%m%d')
            df_news = await self._run_blocking(ak.news_cctv, date=today_str)
            
            if df_news is None or df_news.empty:
                 yesterday_str = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
                 df_news = await self._run_blocking(ak.news_cctv, date=yesterday_str)

            if df_news is not None and not df_news.empty:
                result["telegraph"] = df_news.head(10)['title'].tolist()
            else:
                df_global = await self._run_blocking(ak.stock_info_global_em)
                if df_global is not None and not df_global.empty:
                    result["telegraph"] = df_global.head(10)['æ ‡é¢˜'].tolist()

        except Exception as e:
            logger.warning(f"Failed to fetch macro news: {e}")
        
        ai_keywords = ['äººå·¥æ™ºèƒ½', 'AI', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'ç®—åŠ›', 'å¤§æ¨¡å‹', 'GPU', 'è‹±ä¼Ÿè¾¾', 'åä¸º', 'ç§‘æŠ€', 'æœºå™¨']
        if result["telegraph"]:
            result["ai_tech"] = [
                n for n in result["telegraph"] 
                if any(k in n for k in ai_keywords)
            ][:5]
        
        return result

    async def collect_all(self, portfolio: List[Dict]):
        """
        Main entry point. Orchestrates parallel data fetching.
        """
        logger.info("Starting Batch Data Collection (Dual Source)...")
        
        global_tasks = [
            self.get_market_breadth(),
            self.get_north_funds(),
            self.get_indices(),
            self.get_macro_news()
        ]
        
        # 2. Fetch Spot Data via Fallback
        # This will try Efinance first, then AkShare
        df_all_spot = await self._fetch_with_fallback('fetch_spot_data')
        if df_all_spot is None:
            df_all_spot = pd.DataFrame()
            logger.warning("All sources failed to fetch bulk spot data. Will rely on individual fetch.")
        
        stock_tasks = []
        for stock in portfolio:
            code = stock['code']
            stock_tasks.append(self._fetch_individual_stock_extras(code, stock.get('name', 'Unknown'), df_all_spot))
            
        try:
            global_results = await asyncio.gather(*global_tasks, return_exceptions=True)
            stock_results = await asyncio.gather(*stock_tasks, return_exceptions=True)
        except Exception as e:
            logger.error(f"Critical error during gather: {e}")
            # Try to salvage whatever we have
            global_results = [None] * 4
            stock_results = []

        market_breadth = global_results[0] if global_results and not isinstance(global_results[0], Exception) else "Error"
        north_funds = global_results[1] if global_results and not isinstance(global_results[1], Exception) else 0.0
        indices = global_results[2] if global_results and not isinstance(global_results[2], Exception) else {}
        macro_news = global_results[3] if global_results and not isinstance(global_results[3], Exception) else {"telegraph": [], "ai_tech": []}
        
        valid_stocks = []
        if stock_results:
            valid_stocks = [res for res in stock_results if not isinstance(res, Exception) and isinstance(res, dict) and "error" not in res]

        return {
            "market_breadth": market_breadth,
            "north_funds": north_funds,
            "indices": indices,
            "macro_news": macro_news,
            "stocks": valid_stocks
        }

    # ============================================================
    # Morning Mode: ç›˜å‰å¤–ç›˜æ•°æ®é‡‡é›†
    # ============================================================

    async def get_global_indices(self) -> List[Dict]:
        """
        è·å–éš”å¤œå…¨çƒæŒ‡æ•°ï¼ˆç¾è‚¡/æ’ç”Ÿ/ç¾å…ƒæŒ‡æ•°ç­‰ï¼‰ã€‚
        Uses ak.index_global_spot_em()
        """
        logger.info("Fetching global indices...")
        try:
            df = await self._run_blocking(ak.index_global_spot_em, timeout=10)
            if df is None or df.empty:
                return []

            targets = ['æ ‡æ™®500', 'çº³æ–¯è¾¾å…‹', 'é“ç¼æ–¯', 'æ’ç”ŸæŒ‡æ•°', 'ç¾å…ƒæŒ‡æ•°',
                        'çº³æ–¯è¾¾å…‹100', 'æ—¥ç»225']
            results = []
            for name in targets:
                row = df[df['åç§°'].str.contains(name, na=False)]
                if not row.empty:
                    try:
                        results.append({
                            "name": name,
                            "current": float(row.iloc[0].get('æœ€æ–°ä»·', 0)),
                            "change_pct": float(row.iloc[0].get('æ¶¨è·Œå¹…', 0)),
                            "change_amount": float(row.iloc[0].get('æ¶¨è·Œé¢', 0)),
                        })
                    except (ValueError, KeyError):
                        continue
            return results
        except Exception as e:
            logger.error(f"Failed to fetch global indices: {e}")
            return []

    async def get_commodity_futures(self) -> List[Dict]:
        """
        è·å–éš”å¤œå¤§å®—å•†å“æœŸè´§ï¼ˆé»„é‡‘/ç™½é“¶/é“œ/åŸæ²¹ï¼‰ã€‚
        Uses ak.futures_global_spot_em()
        """
        logger.info("Fetching commodity futures...")
        try:
            df = await self._run_blocking(ak.futures_global_spot_em, timeout=10)
            if df is None or df.empty:
                return []

            targets = ['é»„é‡‘', 'ç™½é“¶', 'é“œ', 'WTIåŸæ²¹', 'å¸ƒä¼¦ç‰¹åŸæ²¹', 'COMEXé“œ']
            results = []
            for name in targets:
                row = df[df['åç§°'].str.contains(name, na=False)]
                if not row.empty:
                    try:
                        results.append({
                            "name": row.iloc[0].get('åç§°', name),
                            "current": float(row.iloc[0].get('æœ€æ–°ä»·', 0)),
                            "change_pct": float(row.iloc[0].get('æ¶¨è·Œå¹…', 0)),
                        })
                    except (ValueError, KeyError):
                        continue
            return results
        except Exception as e:
            logger.error(f"Failed to fetch commodity futures: {e}")
            return []

    async def get_us_treasury_yields(self) -> Dict:
        """
        è·å–ç¾å€ºæ”¶ç›Šç‡ï¼ˆ2Y/10Y/åˆ©å·®ï¼‰ã€‚
        Uses ak.bond_zh_us_rate() with pandas date filtering.
        """
        logger.info("Fetching US treasury yields...")
        try:
            df = await self._run_blocking(ak.bond_zh_us_rate, start_date="2024-01-01", timeout=10)
            if df is None or df.empty:
                return {}

            # Get most recent row
            latest = df.iloc[-1]
            yield_2y = float(latest.get('ç¾å›½å›½å€ºæ”¶ç›Šç‡2å¹´', 0))
            yield_10y = float(latest.get('ç¾å›½å›½å€ºæ”¶ç›Šç‡10å¹´', 0))
            spread = round(yield_10y - yield_2y, 4)

            result = {
                "yield_2y": yield_2y,
                "yield_10y": yield_10y,
                "spread_10y_2y": spread,
            }

            # Calculate change if we have at least 2 rows
            if len(df) >= 2:
                prev = df.iloc[-2]
                prev_10y = float(prev.get('ç¾å›½å›½å€ºæ”¶ç›Šç‡10å¹´', 0))
                result["yield_10y_change"] = round(yield_10y - prev_10y, 4)

            return result
        except Exception as e:
            logger.error(f"Failed to fetch US treasury yields: {e}")
            return {}

    async def _fetch_morning_stock_context(self, code: str, name: str) -> Dict:
        """
        è·å–æŒä»“è‚¡ç¥¨çš„ç›˜å‰ä¸Šä¸‹æ–‡ï¼ˆæ˜¨æ—¥æ”¶ç›˜ä»· + MA20ï¼Œæ— å®æ—¶ä»·ï¼‰ã€‚
        """
        try:
            df_hist = await self._fetch_with_fallback('fetch_prices', code=code, count=30)
            if df_hist is None or df_hist.empty:
                return {"code": code, "name": name, "error": "no_history"}

            # Determine close column
            if 'close' in df_hist.columns:
                close_col = 'close'
            elif 'æ”¶ç›˜' in df_hist.columns:
                close_col = 'æ”¶ç›˜'
            elif 'Close' in df_hist.columns:
                close_col = 'Close'
            else:
                return {"code": code, "name": name, "error": "no_close_column"}

            last_close = float(df_hist.iloc[-1][close_col])

            # Calculate MA20 from pure historical closes (no real-time stitching)
            ma_window = self.config.get('risk_management', {}).get('ma_window', 20)
            closes = df_hist[close_col].tail(ma_window).tolist()
            ma20 = sum(closes) / len(closes) if closes else 0.0
            bias_pct = (last_close - ma20) / ma20 if ma20 > 0 else 0.0

            # Determine MA20 status
            if bias_pct > 0.01:
                ma20_status = "ABOVE"
            elif bias_pct < -0.01:
                ma20_status = "BELOW"
            else:
                ma20_status = "NEAR"

            return {
                "code": code,
                "name": name,
                "last_close": round(last_close, 3),
                "ma20": round(ma20, 3),
                "bias_pct": round(bias_pct, 4),
                "ma20_status": ma20_status,
            }
        except Exception as e:
            logger.error(f"Failed to fetch morning context for {code}: {e}")
            return {"code": code, "name": name, "error": str(e)}

    async def collect_morning_data(self, portfolio: List[Dict]) -> Dict[str, Any]:
        """
        æ—©æŠ¥æ¨¡å¼çš„ä¸»å…¥å£ã€‚å¹¶è¡Œé‡‡é›†å¤–ç›˜æ•°æ® + æ˜¨æ—¥æŒä»“ä¸Šä¸‹æ–‡ã€‚
        """
        logger.info("Starting Morning Pre-Market Data Collection...")

        # Global overnight data tasks
        global_tasks = [
            self.get_global_indices(),
            self.get_commodity_futures(),
            self.get_us_treasury_yields(),
            self.get_macro_news(),
        ]

        # Per-stock historical context
        stock_tasks = [
            self._fetch_morning_stock_context(s['code'], s.get('name', 'Unknown'))
            for s in portfolio
        ]

        try:
            global_results = await asyncio.gather(*global_tasks, return_exceptions=True)
            stock_results = await asyncio.gather(*stock_tasks, return_exceptions=True)
        except Exception as e:
            logger.error(f"Critical error during morning gather: {e}")
            global_results = [None] * 4
            stock_results = []

        global_indices = global_results[0] if not isinstance(global_results[0], Exception) else []
        commodities = global_results[1] if not isinstance(global_results[1], Exception) else []
        us_treasury = global_results[2] if not isinstance(global_results[2], Exception) else {}
        macro_news = global_results[3] if not isinstance(global_results[3], Exception) else {"telegraph": [], "ai_tech": []}

        valid_stocks = [
            res for res in stock_results
            if not isinstance(res, Exception) and isinstance(res, dict) and "error" not in res
        ]

        return {
            "global_indices": global_indices,
            "commodities": commodities,
            "us_treasury": us_treasury,
            "macro_news": macro_news,
            "stocks": valid_stocks,
        }

    async def _fetch_individual_stock_extras(self, code: str, stock_name: str, df_all_spot: pd.DataFrame) -> Dict:
        """
        Fetches History and News for a specific stock using fallback.
        """
        try:
            # 1. Spot Data logic
            current_price = 0.0
            pct_change = 0.0
            volume = 0.0
            turnover_rate = 0.0
            name = stock_name

            if not df_all_spot.empty:
                spot_row = df_all_spot[df_all_spot['code'] == code]
                if not spot_row.empty:
                    try:
                        current_price = float(spot_row.iloc[0]['current_price'])
                        pct_change = float(spot_row.iloc[0]['pct_change'])
                        volume = float(spot_row.iloc[0].get('volume', 0))
                        turnover_rate = float(spot_row.iloc[0].get('turnover_rate', 0))
                    except (ValueError, KeyError, IndexError):
                        pass

            # 2. Try Individual Real-Time Quote (Fallback for Spot)
            # This is critical if bulk spot fetch failed (e.g. Efinance timeout)
            if current_price == 0.0:
                quote = await self._fetch_with_fallback('fetch_single_quote', code=code)
                if quote:
                    try:
                        current_price = float(quote['current_price'])
                        pct_change = float(quote['pct_change'])
                        volume = float(quote.get('volume', 0))
                        turnover_rate = float(quote.get('turnover_rate', 0))
                    except Exception as e:
                        logger.warning(f"Failed to parse quote for {code}: {e}")

            # 3. Fetch Prices (History) via Fallback
            df_hist = await self._fetch_with_fallback('fetch_prices', code=code, count=30)
            if df_hist is None:
                df_hist = pd.DataFrame()
                logger.warning(f"History fetch failed for {code}")

            # Calculate 5-day average volume from history for volume ratio
            # ğŸ”§ ä¿®å¤: æ’é™¤ä»Šæ—¥æ•°æ®ï¼Œç¡®ä¿5æ—¥å‡é‡è®¡ç®—å‡†ç¡®
            avg_volume_5d = 0.0
            if not df_hist.empty:
                # Fallback for current_price if spot failed
                if current_price == 0.0:
                    try:
                        current_price = float(df_hist.iloc[-1]['close'])
                        if len(df_hist) >= 2:
                            prev_close = float(df_hist.iloc[-2]['close'])
                            if prev_close > 0:
                                pct_change = ((current_price - prev_close) / prev_close) * 100
                    except Exception:
                        pass

                # ğŸ”§ ä¿®å¤: è¿‡æ»¤ä»Šæ—¥æ•°æ®åè®¡ç®—5æ—¥å‡é‡
                # é—®é¢˜: df_histå¯èƒ½åŒ…å«ä»Šæ—¥åŠå¤©æ•°æ®ï¼Œå¯¼è‡´å‡é‡è¢«æ‹‰ä½
                # è§£å†³: æŒ‰æ—¥æœŸè¿‡æ»¤ï¼Œæˆ–ä¿å®ˆåœ°æ’é™¤æœ€åä¸€æ¡
                if 'volume' in df_hist.columns and len(df_hist) >= 6:
                    try:
                        # å°è¯•æŒ‰æ—¥æœŸè¿‡æ»¤
                        today = datetime.now().date()
                        if 'date' in df_hist.columns:
                            df_hist_copy = df_hist.copy()
                            df_hist_copy['date'] = pd.to_datetime(df_hist_copy['date'])
                            df_hist_excluding_today = df_hist_copy[
                                df_hist_copy['date'].dt.date < today
                            ]
                            if len(df_hist_excluding_today) >= 5:
                                avg_volume_5d = float(
                                    df_hist_excluding_today.tail(5)['volume'].mean()
                                )
                            else:
                                # æ•°æ®ä¸è¶³ï¼Œç”¨åŸé€»è¾‘
                                avg_volume_5d = float(df_hist.tail(6).head(5)['volume'].mean())
                        else:
                            # æ— æ—¥æœŸåˆ—ï¼Œä¿å®ˆåœ°æ’é™¤æœ€åä¸€æ¡ï¼ˆå¯èƒ½æ˜¯ä»Šæ—¥ï¼‰
                            avg_volume_5d = float(df_hist.tail(6).head(5)['volume'].mean())
                    except Exception as e:
                        logger.warning(f"Failed to calculate avg_volume_5d for {code}: {e}")
                        # é™çº§åˆ°åŸé€»è¾‘
                        avg_volume_5d = float(df_hist.tail(5)['volume'].mean())
                elif 'volume' in df_hist.columns and len(df_hist) >= 5:
                    # æ•°æ®åˆšå¥½5æ¡ï¼Œæ— æ³•æ’é™¤ä»Šæ—¥
                    avg_volume_5d = float(df_hist.tail(5)['volume'].mean())

                df_hist = df_hist.tail(30)
            
            # 3. Fetch News via Fallback
            news_str = await self._fetch_with_fallback('fetch_news', code=code, count=5)
            # news_str returns string separated by ;
            news_list = news_str.split("; ") if news_str else []

            return {
                "code": code,
                "name": name,
                "current_price": current_price,
                "pct_change": pct_change,
                "volume": volume,
                "turnover_rate": turnover_rate,
                "avg_volume_5d": avg_volume_5d,
                "history": df_hist,
                "news": news_list
            }
            
        except Exception as e:
            logger.error(f"Failed individual fetch for {code}: {e}")
            return {"code": code, "error": str(e)}

if __name__ == "__main__":
    # Smoke Test
    class MockConfig:
        config = {'portfolio': [{'code': '600519', 'name': 'èŒ…å°'}]}
    
    start_t = datetime.now()
    collector = DataCollector()
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    result = loop.run_until_complete(collector.collect_all(MockConfig.config['portfolio']))
    
    print(f"Time taken: {datetime.now() - start_t}")
    print(f"Market Breadth: {result['market_breadth']}")
    print(f"North Funds: {result['north_funds']}")
    if result['stocks']:
        print(f"Sample Stock Price: {result['stocks'][0]['current_price']}")
        print(f"Sample Hist Shape: {result['stocks'][0]['history'].shape}")
